{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qkXNgWguPqm7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.auth\n",
    "from google.auth import compute_engine\n",
    "from google.cloud import bigquery\n",
    "from google.colab import auth\n",
    "import google.cloud.bigquery.job as job\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from  __builtin__ import any as b_any\n",
    "from google.cloud import storage\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Create the BQ client. This will ask you to log in the first time.\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Download list of kafka topics to see if any objects are orphaned.\n",
    "table = bigquery.TableReference.from_string(\"data-integration-prod.data_engineering_prod_bq_analysis.orion_kafka_topics\")\n",
    "client = bigquery.Client(project=\"data-integration-prod\")\n",
    "dataframe = client.list_rows(table).to_dataframe()\n",
    "kafka_topics = dataframe.values.tolist()\n",
    "topics_only = map(lambda t: t[0], kafka_topics)\n",
    "\n",
    "# list datasets and number of views/tables\n",
    "print(\"Collecting data on all objects. This may take some time...\")\n",
    "data = []\n",
    "dataset_info = []\n",
    "dataset_count = 0\n",
    "\n",
    "# orion teams\n",
    "orion_datasets = [\"orion\",\"orex\",\"payment\",\"identity\",\"comms\",\"flows\",\"sre\",\"auto_capture\",\n",
    "                  \"tardis\",\"bast\",\"smint\",\"payments\",\"jaws\",\"onboarding\",\"pace\"]\n",
    "\n",
    "project_id = 'data-engineering-prod'\n",
    "client = bigquery.Client(project=project_id)\n",
    "for dataset in [dataset for dataset in client.list_datasets() if any(name in dataset.dataset_id for name in orion_datasets) if not dataset.dataset_id.startswith('u_')]:\n",
    "  dataset_views = list(client.list_tables(dataset=dataset.reference))\n",
    "  dataset_count += 1\n",
    "  for view in dataset_views:\n",
    "      # if the dataset is auto_capture* only the object name will indicate if for orion\n",
    "      if \"auto_capture\" in view.dataset_id and not any(name in view.table_id for name in orion_datasets): continue\n",
    "      # check if the object aligns with a kafka topic\n",
    "      kafka_topic_exists = False\n",
    "      if view.table_id in topics_only: kafka_topic_exists = True\n",
    "      # store the information\n",
    "      data.append([view.full_table_id, view.dataset_id, view.dataset_id.split('_')[0], view.table_type, view.created.strftime('%Y-%m-%d %H:%M:%S'), kafka_topic_exists])\n",
    "\n",
    "  print(dataset.dataset_id + \" contains {:d} views\".format(len(dataset_views)))\n",
    "  dataset_info.append([dataset.dataset_id, len(dataset_views)])\n",
    "\n",
    "print(\"\\n\" + project_id + \" contains {:d} dataset\".format(dataset_count))\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bq_KYmxRPqnL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# UPLOAD VIEWS \n",
    "# write data to newline delimited json file\n",
    "json_file = \"data_engineering_prod_bq_analysis.json\"\n",
    "columns = [\"full_table_id\",\"dataset_id\",\"layer\",\"type\",\"created\",\"kafka_topic_exists\"] #\"query\",\"location\",\"multi_regional\",\"bytes\",\"created\", \"last_modified\"]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "# bq requires newline delimited json so append line break\n",
    "file = open(json_file, \"w\")\n",
    "for row in df.iterrows():\n",
    "  row[1].to_json(file)\n",
    "  file.write(\"\\n\")\n",
    "file.close()\n",
    "\n",
    "# write the results to a gc bucket \n",
    "client = storage.Client(project=project_id)\n",
    "bucket = client.get_bucket(\"data-engineering-prod-bq-analysis\")\n",
    "blob = bucket.blob(json_file)\n",
    "\n",
    "with open('data_engineering_prod_bq_analysis.json', 'rb') as file:\n",
    "  blob.upload_from_file(file)\n",
    "\n",
    "# write data to data-integration-prod\n",
    "# to update the results we will wipe the existing table and replace it\n",
    "project_id = 'data-integration-prod'\n",
    "dataset_id = 'data_engineering_prod_bq_analysis'\n",
    "table_id = 'data_engineering_prod_bq_analysis'\n",
    "full_table_id = project_id + '.' + dataset_id + '.' + table_id\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# remove existing table\n",
    "client.delete_table(full_table_id, not_found_ok=True) \n",
    "\n",
    "# write new table\n",
    "job_config = bigquery.LoadJobConfig(autodetect=True, \n",
    "            source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON)\n",
    "uri = \"gs://data-engineering-prod-bq-analysis/data_engineering_prod_bq_analysis.json\"\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, full_table_id, job_config=job_config)  # Make an API request.\n",
    "load_job.result() \n",
    "\n",
    "# check the number of rows loaded into the table is correct\n",
    "destination_table = client.get_table(full_table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "assert(len(data) == destination_table.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8hGFlSKjPqnU",
    "outputId": "7e12bbc6-3025-4883-f3d0-075e09f3a796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 281 rows.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# UPLOAD DATASET INFO \n",
    "# write data to newline delimited json file\n",
    "json_file = \"data_engineering_prod_bq_dataset_analysis.json\"\n",
    "columns = [\"dataset_id\",\"number_of_objects\"] \n",
    "df = pd.DataFrame(dataset_info, columns=columns)\n",
    "# bq requires newline delimited json so append line break\n",
    "file = open(json_file, \"w\")\n",
    "for row in df.iterrows():\n",
    "  row[1].to_json(file)\n",
    "  file.write(\"\\n\")\n",
    "file.close()\n",
    "\n",
    "# write the results to a gc bucket \n",
    "client = storage.Client(project=project_id)\n",
    "bucket = client.get_bucket(\"data-engineering-prod-bq-analysis\")\n",
    "blob = bucket.blob(json_file)\n",
    "\n",
    "with open('data_engineering_prod_bq_dataset_analysis.json', 'rb') as file:\n",
    "  blob.upload_from_file(file)\n",
    "\n",
    "# write data to data-integration-prod\n",
    "# to update the results we will wipe the existing table and replace it\n",
    "project_id = 'data-integration-prod'\n",
    "dataset_id = 'data_engineering_prod_bq_analysis'\n",
    "table_id = 'data_engineering_prod_bq_dataset_analysis'\n",
    "full_table_id = project_id + '.' + dataset_id + '.' + table_id\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# remove existing table\n",
    "client.delete_table(full_table_id, not_found_ok=True) \n",
    "\n",
    "# write new table\n",
    "job_config = bigquery.LoadJobConfig(autodetect=True, \n",
    "            source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON)\n",
    "uri = \"gs://data-engineering-prod-bq-analysis/data_engineering_prod_bq_dataset_analysis.json\"\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, full_table_id, job_config=job_config)  # Make an API request.\n",
    "load_job.result() \n",
    "\n",
    "# check the number of rows loaded into the table is correct\n",
    "destination_table = client.get_table(full_table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "assert(len(dataset_info) == destination_table.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2PMsLuobVCWS",
    "outputId": "f873d419-5152-4394-a1b0-d00f2b2918d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table number: 118\n"
     ]
    }
   ],
   "source": [
    "# VIEW/TABLE DEPENDENCIES\n",
    "\n",
    "# check which views use which tables\n",
    "views_only = filter(lambda view: view.table_type == \"VIEW\", views)\n",
    "tables_only = filter(lambda table: table.table_type == \"TABLE\", views)\n",
    "print(\"Table number: {}\".format(len(tables_only)))\n",
    "\n",
    "view_table_dependencies = []\n",
    "for table in tables_only:\n",
    "  views_list = []\n",
    "  for view in views_only:\n",
    "    if table.table_id in view.view_query:\n",
    "      views_list.append(view.table_id)\n",
    "  view_table_dependencies.append([table.full_table_id, views_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CbwNsHLdeTJo"
   },
   "outputs": [],
   "source": [
    "# write data to newline delimited json file\n",
    "json_file = \"data_engineering_prod_bq_table_view_dependency_analysis.json\"\n",
    "columns = [\"full_table_id\",\"views_using_table\"]\n",
    "df = pd.DataFrame(view_table_dependencies, columns=columns)\n",
    "# bq requires newline delimited json so append line break\n",
    "file = open(json_file, \"w\")\n",
    "for row in df.iterrows():\n",
    "  row[1].to_json(file)\n",
    "  file.write(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qvrw4IurfELf"
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# write the results to a gc bucket \n",
    "client = storage.Client(project=project_id)\n",
    "bucket = client.get_bucket(\"data-engineering-prod-bq-analysis\")\n",
    "blob = bucket.blob(json_file)\n",
    "\n",
    "with open('data_engineering_prod_bq_table_view_dependency_analysis.json', 'rb') as file:\n",
    "  blob.upload_from_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wgx6G9r8fNKH",
    "outputId": "6018c165-023b-48dc-e59b-eb8fc1c870c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 118 rows.\n"
     ]
    }
   ],
   "source": [
    "# write data to data-integration-prod\n",
    "# to update the results we will wipe the existing table and replace it\n",
    "project_id = 'data-integration-prod'\n",
    "dataset_id = 'data_engineering_prod_bq_analysis'\n",
    "table_id = 'data_engineering_prod_bq_table_view_dependency_analysis'\n",
    "full_table_id = project_id + '.' + dataset_id + '.' + table_id\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# remove existing table\n",
    "client.delete_table(full_table_id, not_found_ok=True)\n",
    "\n",
    "# write new table\n",
    "job_config = bigquery.LoadJobConfig(autodetect=True, \n",
    "            source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON)\n",
    "uri = \"gs://data-engineering-prod-bq-analysis/data_engineering_prod_bq_table_view_dependency_analysis.json\"\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, full_table_id, job_config=job_config)  # Make an API request.\n",
    "load_job.result() \n",
    "\n",
    "# check the number of rows loaded into the table is correct\n",
    "destination_table = client.get_table(full_table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "assert(len(view_table_dependencies) == destination_table.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-_c73Vv1PTN"
   },
   "outputs": [],
   "source": [
    "### COST ANALYSIS ###\n",
    "# This section is for the cost analysis of existing tables and views in the project\n",
    "\n",
    "# https://cloud.google.com/bigquery/pricing\n",
    "\n",
    "# - cost of storage (bytes)\n",
    "# - region/multi\n",
    "# - partitioned\n",
    "# - expiration\n",
    "\n",
    "# collect relevant data\n",
    "cost_data = []\n",
    "\n",
    "# https://cloud.google.com/bigquery/docs/locations\n",
    "# indicates \"EU\" & \"US\" are multi-regional locations\n",
    "multi_regional = [\"EU\", \"US\"]\n",
    "\n",
    "for view in views:\n",
    "  multi = True if view.location in multi_regional else False\n",
    "\n",
    "  d = [view.full_table_id,\n",
    "      view.location, \n",
    "      multi,\n",
    "      view.num_bytes, \n",
    "      view.expires.strftime('%Y-%m-%d %H:%M:%S') if view.expires != None else None,\n",
    "      view.partitioning_type,\n",
    "      view.partition_expiration.strftime('%Y-%m-%d %H:%M:%S') if view.partition_expiration != None else None,\n",
    "      ]\n",
    "  cost_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8dotILs51PTR"
   },
   "outputs": [],
   "source": [
    "# write data to newline delimited json file\n",
    "json_file = \"data_engineering_prod_bq_cost_analysis.json\"\n",
    "columns = [\"full_table_id\",\"location\",\"multi\",\"bytes\",\"expiration\",\"partition\",\"partition_expiration\"]\n",
    "df = pd.DataFrame(cost_data, columns=columns)\n",
    "# bq requires newline delimited json so append line break\n",
    "file = open(json_file, \"w\")\n",
    "for row in df.iterrows():\n",
    "  row[1].to_json(file)\n",
    "  file.write(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yEKQEco31PTT"
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# write the results to a gc bucket \n",
    "client = storage.Client(project=project_id)\n",
    "bucket = client.get_bucket(\"data-engineering-prod-bq-analysis\")\n",
    "blob = bucket.blob(json_file)\n",
    "\n",
    "with open('data_engineering_prod_bq_cost_analysis.json', 'rb') as file:\n",
    "  blob.upload_from_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9q7GRT5G1PTW",
    "outputId": "830f74f6-d56a-45ca-f0ed-7585be34e5b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 281 rows.\n"
     ]
    }
   ],
   "source": [
    "# write data to data-integration-prod\n",
    "# to update the results we will wipe the existing table and replace it\n",
    "project_id = 'data-integration-prod'\n",
    "dataset_id = 'data_engineering_prod_bq_analysis'\n",
    "table_id = 'data_engineering_prod_bq_cost_analysis'\n",
    "full_table_id = project_id + '.' + dataset_id + '.' + table_id\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# remove existing table\n",
    "client.delete_table(full_table_id, not_found_ok=True)\n",
    "\n",
    "# write new table\n",
    "job_config = bigquery.LoadJobConfig(autodetect=True, \n",
    "            source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON)\n",
    "uri = \"gs://data-engineering-prod-bq-analysis/data_engineering_prod_bq_cost_analysis.json\"\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, full_table_id, job_config=job_config)  # Make an API request.\n",
    "load_job.result() \n",
    "\n",
    "# check the number of rows loaded into the table is correct\n",
    "destination_table = client.get_table(full_table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "assert(len(cost_data) == destination_table.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7P2PxHWuDzKx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "orion_data_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
