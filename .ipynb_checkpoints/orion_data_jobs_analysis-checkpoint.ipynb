{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qkXNgWguPqm7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import google.auth\n",
    "from google.auth import compute_engine\n",
    "from google.cloud import bigquery\n",
    "from google.colab import auth\n",
    "\n",
    "# Create the BQ client. This will ask you to log in the first time.\n",
    "\n",
    "project_id = 'andromeda-data-nonprod'\n",
    "auth.authenticate_user()\n",
    "client = bigquery.Client(project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yVcItLGiPqnA",
    "outputId": "a889a7c5-72b6-483b-c026-27a578819946"
   },
   "outputs": [],
   "source": [
    "# list datasets and number of views/tables\n",
    "print(\"Collecting data on all views in \" + project_id + \". This may take some time...\")\n",
    "views = []\n",
    "dataset_count = 0\n",
    "for dataset in client.list_datasets():\n",
    "  print(\"\\nCollecting views for: \" + dataset.dataset_id)  \n",
    "  dataset_views = list(client.list_tables(dataset=dataset.reference))\n",
    "  dataset_count += 1\n",
    "  for view in dataset_views:\n",
    "    try:\n",
    "      dataset_ref = bigquery.DatasetReference(project_id, dataset.dataset_id)\n",
    "      view_ref = dataset_ref.table(view.table_id)\n",
    "      v = client.get_table(view_ref)\n",
    "      views.append(v)\n",
    "\n",
    "    except Exception, err:\n",
    "      print(err)\n",
    "      pass\n",
    "\n",
    "  print(dataset.dataset_id + \" contains {:d} views\".format(len(dataset_views)))\n",
    "\n",
    "print(\"\\n\" + project_id + \" contains {:d} dataset\".format(dataset_count))\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "6AskdpkiPqnI",
    "outputId": "d456dbcc-0c05-45f9-dc2f-079c34f5824e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example table/view and query:\n",
      "\n",
      "View: data-engineering-prod:landing_andromeda.energy_contracts_consumption_charge_generated_v2\n",
      "\n",
      "View Query:\n",
      "#standardSQL\n",
      "SELECT\n",
      "  `kafkaData`,\n",
      "  `unionRecord`,\n",
      "  `_PARTITIONTIME` `PARTITIONTIME`\n",
      "FROM\n",
      "  `data-engineering-prod.auto_capture_v2.energy_contracts_consumption_charge_generated_v2`\n",
      "\n",
      "Type: VIEW\n"
     ]
    }
   ],
   "source": [
    "# Example data from table/view\n",
    "print(\"Example table/view and query:\")\n",
    "print(\"\\nView: {}\".format(views[0].full_table_id))\n",
    "print(\"\\nView Query:\\n{}\".format(views[0].view_query))\n",
    "print(\"\\nType: {}\".format(views[0].table_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1cIs3cI5PqnW"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "import google.cloud.bigquery.job as job\n",
    "from collections import Counter\n",
    "\n",
    "### JOBS ANALYSIS ###\n",
    "# Performs analysis on which objects are being used,\n",
    "# how often and by whom.\n",
    "\n",
    "project_id = 'andromeda-data-nonprod' # this will need to be updated to 'data-engineering-prod'\n",
    "print(\"Starting jobs processing...\")\n",
    "# for 30 days ago (min_creation_time)\n",
    "thirty_days = datetime.datetime.now(pytz.UTC) - datetime.timedelta(30)\n",
    "sixty_days = datetime.datetime.now(pytz.UTC) - datetime.timedelta(60)\n",
    "ninety_days = datetime.datetime.now(pytz.UTC) - datetime.timedelta(90)\n",
    "jobs = list(client.list_jobs(project=project_id, min_creation_time=ninety_days))\n",
    "\n",
    "# for each object\n",
    "  # how many jobs are for that object: count\n",
    "  # which users are querying that object: users[]\n",
    "  # when was the last query on that object: datetime\n",
    "# => [table_id, count, users[], datetime]\n",
    "jobs_data = []\n",
    "for view in views:\n",
    "  # filter for query jobs\n",
    "  query_jobs = filter(lambda x: type(x) is job.QueryJob, jobs)\n",
    "  query_jobs_for_view = filter(lambda x: view.table_id in x.query, query_jobs)\n",
    "  \n",
    "  queries_thirty_days = filter(lambda x: x.created > thirty_days, query_jobs_for_view)\n",
    "  queries_sixty_days= filter(lambda x: x.created > sixty_days, query_jobs_for_view)\n",
    "  queries_ninety_days = filter(lambda x: x.created > ninety_days, query_jobs_for_view)\n",
    "\n",
    "  users_30_days = map(lambda x: x.user_email, queries_thirty_days)\n",
    "  users_60_days = map(lambda x: x.user_email, queries_sixty_days)\n",
    "  users_90_days = map(lambda x: x.user_email, queries_ninety_days)\n",
    "\n",
    "  lastQuery = next(iter(sorted(query_jobs_for_view, key = lambda x: x.created, reverse=True)), None)\n",
    "  \n",
    "  d = [\"QueryJob\",\n",
    "       view.full_table_id, \n",
    "       view.dataset_id.split('_')[0], \n",
    "       len(queries_thirty_days), \n",
    "       len(queries_sixty_days),\n",
    "       len(queries_ninety_days),\n",
    "       list(Counter(users_30_days).most_common())[0][0] if len(users_30_days) > 0 else \"No user data\", \n",
    "       list(Counter(users_60_days).most_common())[0][0] if len(users_60_days) > 0 else \"No user data\", \n",
    "       list(Counter(users_90_days).most_common())[0][0] if len(users_90_days) > 0 else \"No user data\", \n",
    "       lastQuery.created.strftime('%Y-%m-%d %H:%M:%S') if lastQuery != None else None]  \n",
    "\n",
    "  jobs_data.append(d)\n",
    "\n",
    "  # filter for load jobs - not sure these are required\n",
    "#  loadJobs = filter(lambda x: type(x) is job.LoadJob, jobs)\n",
    "#  loadJobsForView = filter(lambda x: view.table_id == x.destination, loadJobs)\n",
    "#  users = map(lambda x: x.user_email, loadJobsForView)\n",
    "#  loaded_thirty_days = filter(lambda x: x.created > thirty_days, queryJobsForView)\n",
    "#  loaded_sixty_days= filter(lambda x: x.created > sixty_days, queryJobsForView)\n",
    "#  loaded_ninety_days = filter(lambda x: x.created > ninety_days, queryJobsForView)\n",
    "#  lastQuery = next(iter(sorted(loadJobsForView, key = lambda x: x.created, reverse=True)), None) \n",
    "#  d = [\"LoadJob\",\n",
    "#      view.full_table_id, \n",
    "#      view.dataset_id.split('_')[0], \n",
    "#      len(queries_thirty_days), \n",
    "#      len(queries_sixty_days),\n",
    "#      len(queries_ninety_days),\n",
    "#      list(Counter(users).most_common()) if len(users) > 0 else \"No user data\", \n",
    "#      lastQuery.created.strftime('%Y-%m-%d %H:%M:%S') if lastQuery != None else None]  \n",
    "\n",
    "#  jobs_data.append(d)\n",
    "\n",
    "\n",
    "print(\"Jobs processing finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to newline delimited json file\n",
    "json_file = \"data_engineering_prod_bq_jobs_analysis.json\"\n",
    "columns = [\"type\",\"full_table_id\",\"layer\",\"queries_30_days\",\"queries_60_days\",\"queries_90_days\",\"most_common_user_30\",\"most_common_user_60\",\"most_common_user_90\",\"last_query_time\"]\n",
    "df = pd.DataFrame(jobs_data, columns=columns)\n",
    "# bq requires newline delimited json so append line break\n",
    "file = open(json_file, \"w\")\n",
    "for row in df.iterrows():\n",
    "  row[1].to_json(file)\n",
    "  file.write(\"\\n\")\n",
    "file.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# write the results to a gc bucket \n",
    "client = storage.Client(project=project_id)\n",
    "bucket = client.get_bucket(\"data-engineering-prod-bq-analysis\")\n",
    "blob = bucket.blob(json_file)\n",
    "\n",
    "with open('data_engineering_prod_bq_jobs_analysis.json', 'rb') as file:\n",
    "  blob.upload_from_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to data-integration-prod\n",
    "# to update the results we will wipe the existing table and replace it\n",
    "project_id = 'data-integration-prod'\n",
    "dataset_id = 'data_engineering_prod_bq_analysis'\n",
    "table_id = 'data_engineering_prod_bq_jobs_analysis'\n",
    "full_table_id = project_id + '.' + dataset_id + '.' + table_id\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# remove existing table\n",
    "client.delete_table(full_table_id, not_found_ok=True)\n",
    "\n",
    "# write new table\n",
    "job_config = bigquery.LoadJobConfig(autodetect=True, \n",
    "            source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON)\n",
    "uri = \"gs://data-engineering-prod-bq-analysis/data_engineering_prod_bq_jobs_analysis.json\"\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, full_table_id, job_config=job_config)  # Make an API request.\n",
    "load_job.result() \n",
    "\n",
    "# check the number of rows loaded into the table is correct\n",
    "destination_table = client.get_table(full_table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "assert(len(jobsData) == destination_table.num_rows)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "orion_data_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
