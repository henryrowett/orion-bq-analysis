{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qkXNgWguPqm7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import google.auth\n",
    "from google.auth import compute_engine\n",
    "from google.cloud import bigquery\n",
    "from google.colab import auth\n",
    "\n",
    "# Create the BQ client. This will ask you to log in the first time.\n",
    "\n",
    "project_id = 'data-engineering-prod'\n",
    "auth.authenticate_user()\n",
    "client = bigquery.Client(project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yVcItLGiPqnA",
    "outputId": "d6040198-b055-4646-dd6b-adf8220d5c16"
   },
   "outputs": [],
   "source": [
    "# list datasets and number of views/tables\n",
    "print(\"Collecting data on all views in \" + project_id + \". This may take some time...\")\n",
    "views = []\n",
    "dataset_count = 0\n",
    "for dataset in client.list_datasets():\n",
    "  if dataset.dataset_id.startswith('u_'): continue\n",
    "  dataset_views = list(client.list_tables(dataset=dataset.reference))\n",
    "  dataset_count += 1\n",
    "  for view in dataset_views:\n",
    "      views.append(view)\n",
    "\n",
    "  print(dataset.dataset_id + \" contains {:d} views\".format(len(dataset_views)))\n",
    "\n",
    "print(\"\\n\" + project_id + \" contains {:d} dataset\".format(dataset_count))\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1cIs3cI5PqnW",
    "outputId": "208d27da-8503-4d13-f3b6-13903438543d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting jobs processing...\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import google.cloud.bigquery.job as job\n",
    "from collections import Counter\n",
    "from  __builtin__ import any as b_any\n",
    "from google.cloud import storage\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "### JOBS ANALYSIS ###\n",
    "# Performs analysis on which objects are being used,\n",
    "# how often and by whom.\n",
    "\n",
    "# submit query as user in data-integration-prod\n",
    "project_id = 'data-integration-prod' \n",
    "client = bigquery.Client(project=project_id)\n",
    "print(\"Starting jobs processing...\")\n",
    "start = timer()\n",
    "\n",
    "# for each object\n",
    "jobs_data = []\n",
    "error_data = []\n",
    "cost_data = []\n",
    "\n",
    "project_id_for_query = \"data-engineering-prod\"\n",
    "\n",
    "today = datetime.now().strftime('%Y%m%d')\n",
    "startDate = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')\n",
    "\n",
    "for view in views:\n",
    "\n",
    "  # we now want to use the logs dump to return jobs only for the specified view\n",
    "  query = \"\"\"\n",
    "      select protopayload_auditlog.authenticationInfo.principalEmail as email,\n",
    "      timestamp as time,\n",
    "      protopayload_auditlog.servicedata_v1_bigquery.jobInsertRequest.resource.jobName.location as location,\n",
    "      protopayload_auditlog.servicedata_v1_bigquery.jobInsertRequest.resource.jobConfiguration.query.query as query,\n",
    "      protopayload_auditlog.servicedata_v1_bigquery.jobInsertRequest.resource.jobStatus.error.message\n",
    "      from `data-engineering-prod.logs_bigquery.cloudaudit_googleapis_com_data_access_*`\n",
    "      where REGEXP_CONTAINS(protopayload_auditlog.servicedata_v1_bigquery.jobInsertRequest.resource.jobConfiguration.query.query, \"{0}\") \n",
    "      and protopayload_auditlog.servicedata_v1_bigquery.jobInsertRequest.resource.jobConfiguration.dryRun is null\n",
    "      and protopayload_auditlog.servicedata_v1_bigquery.jobInsertResponse.resource.jobStatus.state = \"DONE\"\n",
    "      AND _TABLE_SUFFIX BETWEEN '{1}' and '{2}'\n",
    "    \"\"\".format(view.full_table_id, today, startDate)\n",
    "  query_job = client.query(query)\n",
    "\n",
    "  users = map(lambda x: x[0], list(query_job))\n",
    "  most_common_user = list(Counter(users).most_common())[0][0] if len(users) > 0 else \"No user data\"\n",
    "  last_query_mapped = map(lambda x: x[1], list(query_job))\n",
    "  last_query_filter = filter(lambda x: x != None, last_query_mapped)\n",
    "  last_query = next(iter(sorted(last_query_filter, reverse=True)), None)\n",
    "  \n",
    "  d = [view.full_table_id, \n",
    "      view.dataset_id,\n",
    "      view.table_id,\n",
    "      view.dataset_id.split('_')[0], \n",
    "      len(list(query_job)),\n",
    "      most_common_user,\n",
    "      b_any(\"serviceaccount.com\" in user for user in Counter(users).keys()),\n",
    "      b_any(\"ovoenergy.com\" in user for user in Counter(users).keys()),\n",
    "      last_query.strftime(\"%Y%m%d\") if last_query != None else None]  \n",
    "\n",
    "  jobs_data.append(d)\n",
    "\n",
    "  # if [\"EU\",\"US\"] => multi-region (this will be every query basically so not high importance)\n",
    "  # if ORDER BY => requires single node\n",
    "  # any errors?\n",
    "  # log the query\n",
    "  for job in list(query_job): \n",
    "    if (job[4] != None) | ('ORDER BY' in job[3]):\n",
    "      error = [view.full_table_id, \n",
    "               job[2],\n",
    "               job[4],\n",
    "               job[3],\n",
    "               'ORDER BY' in job[3]]\n",
    "      print(error)\n",
    "\n",
    "end = timer()\n",
    "print(\"Jobs processing finished in {} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Google Cloud client library and JSON library\n",
    "from google.cloud import storage\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "\n",
    "project_id='data-integration-prod'\n",
    "# Instantiate a Google Cloud Storage client and specify required bucket and file\n",
    "storage_client = storage.Client(project=project_id)\n",
    "bucket = storage_client.get_bucket(\"data-engineering-prod-bq-analysis\")\n",
    "blob = bucket.blob(\"data_engineering_prod_bq_jobs_analysis_30_days.json\")\n",
    "\n",
    "# Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "json_file = \"data_engineering_prod_bq_analysis_30_days_update_script.json\"\n",
    "\n",
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(\"data-integration-prod.data_engineering_prod_bq_analysis.data_engineering_prod_bq_jobs_analysis_30_days\")\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataframe = client.list_rows(table).to_dataframe()\n",
    "data = dataframe.values.tolist()\n",
    "print(len(data))\n",
    "\n",
    "# update data and reupload to bucket\n",
    "for entry in data:\n",
    "  for job in jobs_data:\n",
    "    if job[0] in entry[6]: # check full table name\n",
    "        entry[0] = today # update last query\n",
    "        entry[3] += job[4] # increase query count\n",
    "        if job[6] == True & entry[1] == False: # service account\n",
    "          entry[1] = True\n",
    "        if job[7] == True & entry[5] == False: # retail account\n",
    "          entry[5] = True\n",
    "        \n",
    "    else:\n",
    "      d = [today,\n",
    "           job[6], # service account\n",
    "           job[5], # most common user\n",
    "           job[4], # number of queries\n",
    "           job[3], # layer\n",
    "           job[7], # retail account\n",
    "           job[0], # full table id\n",
    "           job[2], # table id\n",
    "           job[1]], # dataset id\n",
    "      data.append(d) \n",
    "\n",
    "\n",
    "# write data to data-integration-prod\n",
    "# to update the results we will wipe the existing table and replace it\n",
    "#project_id = 'data-integration-prod'\n",
    "#dataset_id = 'data_engineering_prod_bq_analysis'\n",
    "#table_id = 'data_engineering_prod_bq_jobs_analysis_30_days'\n",
    "#full_table_id = project_id + '.' + dataset_id + '.' + table_id\n",
    "#print(\"Updating \" + full_table_id)\n",
    "\n",
    "#client = bigquery.Client(project=project_id)\n",
    "\n",
    "# remove existing table\n",
    "#client.delete_table(full_table_id, not_found_ok=True)\n",
    "\n",
    "# write new table\n",
    "#job_config = bigquery.LoadJobConfig(autodetect=True, \n",
    "#            source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON)\n",
    "#uri = \"gs://data-engineering-prod-bq-analysis/data_engineering_prod_bq_jobs_analysis_30_days.json\"\n",
    "#load_job = client.load_table_from_uri(\n",
    "#    uri, full_table_id, job_config=job_config)  # Make an API request.\n",
    "#load_job.result() \n",
    "\n",
    "# check the number of rows loaded into the table is correct\n",
    "#destination_table = client.get_table(full_table_id)\n",
    "#print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "#assert(len(jobs_data) == destination_table.num_rows)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "orion_data_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
